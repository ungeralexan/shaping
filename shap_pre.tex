\documentclass[12pt]{article}

\usepackage[a4paper, left=2.5cm, top=2.5cm, right=2.5cm, bottom=2cm]{geometry}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage{hyperref} % For hyperlinks
\usepackage{graphicx} % For images
\usepackage{tabularx} % For flexible-width columns
\usepackage{amsmath}
\usepackage{amssymb}
\setstretch{1.5}
\begin{document}
\section{SHAP as model agnostic solution}
The foundation of SHAP lies in cooperative game theory, specifically drawing upon the Shapley values, a concept developed by Lloyd Shapley in 1953. Originally intended to fairly distribute payouts among players based on their contributions to the overall game, Shapley values are employed in SHAP to quantify the contribution of each feature to a prediction, effectively adapting this game theory concept to the field of machine learning. To further elucidate this theory, the next section will present an illustrative game to demonstrate the calculation of Shapley values.
	\subsection{Shapley values an example}
    Let's consider a concrete example to illustrate a coalitional game.
	Imagine three students—Elina, Tom, and Lucy—who share an apartment. All of them enjoy watching TV, though their individual interest levels vary. Together, they subscribe to Amazon Prime. 
	Besides the fixed monthly subscription cost, they often rent movies, which adds to the final bill. This month, the total expense reaches \$50 due to these additional rentals. Faced with this cost, the trio must decide how to fairly split the bill. Even though the cost represents a negative payout, this situation aligns well with the principles of a coalitional game. To ensure a fair distribution of costs, it's important to understand what the subscription would cost for various coalitions of students. For example, we might consider how much Elina would pay if she were living alone, or what the cost would be for Lucy and Tom if they were the only ones sharing the apartment.
	Let's assume that the friends have diverse viewing habits which impact their portion of the bill. For instance, Elina, who rents movies daily, would face a cost of \$30 when living alone. Adding Tom to the mix, known for his weekend binge-watching, increases the total expense to \$45. Including Lucy, who occasionally watches documentaries, brings the total to \$50. To systematically explore these variations, we will compile the costs for all possible coalitions Table \ref{tab:subscription_costs} below:
	\begin{table}[htbp]
		\centering
		\caption{Subscription costs}
		\begin{tabular}{lll}
			\toprule
			Coalition & Cost & Explanation \\
			\midrule
			$\emptyset$ & \$0 & No subscription, no costs. \\
			\{Elina\} & \$30 & High costs from daily movie rentals. \\
			\{Tom\} & \$20 & Lower costs, primarily weekend watching. \\
			\{Lucy\} & \$10 & Minimal costs, watches documentaries occasionally. \\
			\{Elina, Tom\} & \$45 & Increased costs from both's frequent viewing. \\
			\{Elina, Lucy\} & \$40 & Costs moderated by mixed viewing habits. \\
			\{Tom, Lucy\} & \$25 & Moderate costs from their less frequent usage. \\
			\{Elina, Tom, Lucy\} & \$50 & Total cost reflects full usage by all. \\
			\bottomrule
		\end{tabular}%
		\label{tab:subscription_costs}%
	\end{table}%
	In our example, the symbol $\emptyset$ represents a coalition without any members, indicating that no subscription has been made. To further investigate how much each student contributes to the cost of the subscription, Table \ref{tab:coalition_costs} will now provide the marginal contributions (MC) of each student.
	\begin{table}[htbp]
		\centering
		\caption{Coalition Costs and Marginal Contributions (MC)}
		\begin{tabular}{lllll}
			\toprule
			Coalition Before Joining & Joining Member & Cost Before & Cost After Joining & MC \\
			\midrule
			$\emptyset$ & Elina & \$0 & \$30 & \$30 \\
			$\emptyset$ & Tom & \$0 & \$20 & \$20 \\
			$\emptyset$ & Lucy & \$0 & \$10 & \$10 \\
			\{Elina\} & Tom & \$30 & \$45 & \$15 \\
			\{Elina\} & Lucy & \$30 & \$40 & \$10 \\
			\{Tom\} & Elina & \$20 & \$45 & \$25 \\
			\{Tom\} & Lucy & \$20 & \$25 & \$5 \\
			\{Lucy\} & Elina & \$10 & \$40 & \$30 \\
			\{Lucy\} & Tom & \$10 & \$25 & \$15 \\
			\{Elina, Tom\} & Lucy & \$45 & \$50 & \$5 \\
			\{Elina, Lucy\} & Tom & \$40 & \$50 & \$10 \\
			\{Tom, Lucy\} & Elina & \$25 & \$50 & \$25 \\
			\bottomrule
		\end{tabular}%
		\label{tab:coalition_costs}%
	\end{table}%
	The marginal contribution (MC) of each individual is calculated as the difference between the value of a coalition including the individual and the value of the coalition without that individual. To accurately assess these contributions using Shapley values, it is necessary to consider all possible permutations of the coalition members—Elina, Tom, and Lucy. Given that there are three members, there are $ 3! = 3 * 2 * 1 = 6$ possible permutations. These permutations are: 
	\begin{itemize}
		\item Elina, Tom, Lucy
		\item Elina, Lucy, Tom
		\item Tom, Elina, Lucy
		\item Tom, Lucy, Elina
		\item Lucy, Tom, Elina
		\item Lucy, Elina, Tom
	\end{itemize}
	This enumeration allows us to systematically calculate the Shapley value for each individual by averaging their marginal contributions across all permutations. 
	For instance, Elina's Shapley value is calculated by averaging her contributions across all permutations: she joins first in 2 out of 6 cases, joins after Tom or Lucy individually in 1 out of 6 cases each, and joins last after both in 2 out of 6 cases. The Shapley value for Elina is computed as follows:
	$$
	\frac{1}{6} \left( 
	\underset{\text{E to } \emptyset}{2 \cdot \$30} + 
	\underset{\text{E to T}}{1 \cdot \$25} + 
	\underset{\text{Eto L}}{1 \cdot \$30} + 
	\underset{\text{E to T,L}}{2 \cdot \$25} 
	\right) = \$27.50
	$$
	We multiply by $\frac{1}{6}$ because there are 6 permutations, each weighted equally. If we perform similar calculations for Tom and Lucy, we find Shapley values of \$15 for Tom and \$7.50 for Lucy (details can be found in Equation \ref{eq:tom_shapley} in the Appendix). The individual contributions of \$7.50, \$15, and \$27.50 sum to the total cost of \$50. 
	Based on the calculations of Shapley values in the subscription cost example, we can now formalize the Shapley value formula for the general case.
	\begin{equation}
		\phi_j = \sum_{S \subseteq N\backslash\{j\}} \frac{|S|!(N - |S| - 1)!}{N!}(v(S \cup \{j\}) - v(S)) 
	\end{equation}
In this sense \(N\) is the set of all players in the game. \(S\) is a subset of \(N\) that does not include player \(j\). \(v(S)\) is the value function that assigns a value to each coalition \(S\).
 \(v(S \cup \{j\}) - v(S)\) represents the marginal contribution of player \(j\) to the coalition \(S\) and the factor \(\frac{|S|!(|N| - |S| - 1)!}{|N|!}\) is a weighting coefficient that accounts for the number of permutations in which the coalition \(S\) can be formed before \(j\) joins.
The final Shapley value $\phi$ is then interpreted as the wieghted average of a players marginal contribution to all possible coalitions. 
\subsection{The four axioms that ensure fairness}
Shapley values distinguish themselves from other methods by ensuring that payouts are distributed among players in a fair manner. Given the subjective nature of Fariness, Lloyd Shapley proposed four foundational axioms that outline a universally fair distribution mechanism. 
These 4 central axioms are Efficiency, Symmetry, Dummy and Aditivity.\\
\textbf{Efficiency}: This axioms guarantees that the individual contributions of all features sum up to the difference between the prediction for an instance i and the expected prediction across the entire dataset:
\begin{equation}
\sum_{j=1}^{N} \phi^{(i)}_j = f(x^{(i)}) - E(f(X))
\end{equation}
\textbf{Symmetry}: This axioms states that if two features j and k contributed in the same way to all possible coalitions, then they should get the same Shapley values:
\begin{align*}
\text{If} \, v(S \cup \{j\})  = v(S \cup \{k\})\\
\text{for all coalations:} \, {S \subseteq {1,\ldots,N}\backslash\{j,k\}}\\
\text{then:} \,  \phi^{(i)}_j  = \phi^{(i)}_k
\end{align*}
\text{Dummy}: This axiom states that for any two predictive scenarios a and be the Shapley value of a feature in the combined game is the sum of their Shapley values from each indvidual game:
\begin{align*}
	\text{If} \, v(S \cup \{j\})  = v(S)\\
	\text{for all coalations:} \, {S \subseteq {1,\ldots,N}}\\
	\text{then:} \,  \phi^{(i)}_j  = 0
\end{align*}
\textbf{Additivity}: The last Axiom states that for any two games the Shapley value of a feature is the sum of the Shapley value of each individual prediction:
\begin{align}
\phi^{(a)}_j + \phi^{(b)}_j = \phi^{(a+b)}_j
\end{align}

\subsection{From Shapley values to SHAP}
When Shapley values are applied to the explanation of machine learning model predictions, they manifest as SHAP (SHapley Additive exPlanations).
The SHAP framework developed by Lundberg and Lee in 2017 focuses on quantifying the contribution of each feature to the deviation of a particular prediction for an instance i from the average prediction made by the model across all data.
In this analogy, each feature in our model represents a "player" in the coalition game, and the "payout" corresponds to the prediction output. 
In contrast to the theoretical framework of the Shapley values where the payouts for all coalitions were predetermined, in machine learning the direct knowledge of the prediction for every feature coalition is typically unavailable. Instead we rely on a predictive model capable of estimating outcomes based on a complete set of feature values.
To address this, SHAP values are derived using the expected value of the model's prediction, which accounts for the probability distribution of unspecified features. This approach helps in approximating the contribution of each feature to the prediction, aligning with the fair allocation principles of Shapley values.
The marginal contribution of a feature j in the SHAP framework can be shown as:
$$v(S \cup j) - v(S) = \int f(x_{S \cup j}^{(i)} \cup x_{C\setminus j})dP_{X_{C\setminus j}} \\
- \int f(x_S^{(i)} \cup x_C)dP_{X_C}$$
Here S contains a subset of known feature values and $X_C$ represents  the features that are not in S. The function f denotes the model's prediction function. $X_{C\setminus j}$ is the set of absent features excluding, representing the complement to $S \cup j$.
SHAP’s approach involves integrating over the distribution of these "missing" features in $X_C$ to calculate the change in a prediction, capturing the average marginal contribution of a feature across all possible values of the other features. This is done for all possible subsets S from which feature j is excluded.
The SHAP value for feature j for instance i, denoted as $\phi^{i}_j$, is then calculated by eighing the contributions across all permutations of features as follows.
$$	\phi^{i}_j = \sum_{S \subseteq N\backslash\{j\}} \frac{|S|!(p - |S| - 1)!}{p!} * ( \int f(x_{S \cup j}^{(i)} \cup x_{C\setminus j})dP_{X_{C\setminus j}} \\
- \int f(x_S^{(i)} \cup x_C)dP_{X_C})$$ 
In this equation, p is the total number of features, and N is the full set of features. The formula calculates the expected change in the model's output when feature j is added to any subset S of features, integrating over the distributions of the remaining features in $X_C$. 
This construct allows the SHAP value to be interpreted as the average marginal contribution of a feature's value $x^{i}_j $ across all possible coalitions of features.
The SHAP framework same as the SHapley values also satisfy the above mentioned axioms. In comparison to the SHapley value framework, SHAP only defines 3 properties, called Local Accuracy, Missingness andConsitency. Those axioms are very similar to teh axioms of the SHapley value framework, an explicit definition and mathematical deriviton of them can be found in the Appendix.

\subsection{Problems in practice}
 In practical applications within machine learning environments, we often lack explicit knowledge about the distributions of the missing features denoted as $X_C$. This lack of distributional information can make it challenging to directly integrate over these features, as required by the theoretical computation of SHAP values.
 Furthermore, the complexity increases with the number of features in a model. Specifically, the number of potential feature coalitions increases exponentially, as $(2^n)$ where n is the number of features. 
 This makes the computation of marginal contributions for each feature across all coalitions a time-consuming, if not intractable, task.
Given these computational constraints, it is often impractical to calculate exact SHAP values directly. However, various estimation techniques and algorithmic optimizations have been developed to approximate SHAP values efficiently, enabling the practical application of this concept for model interpretability in machine learning.

\subsection{Kernel SHAP}
One effective approximation method for SHAP values is Kernel SHAP, which was the original estimation framework proposed by Lundberg and Lee in 2017. Kernel SHAP addresses the challenge of unknown distributions of features by using a reference dataset to represent typical values. This method samples from the marginal distribution of the features, effectively decoupling the dependencies between absent and present features. This approach simplifies the computation by ignoring any dependency structure between the features, which might not always capture interactions accurately but significantly eases the computational load.
Additionally, Instead of computing the contribution for all $2^n$ possible values Kernel SHAP strategically samples these coalitions. This sampling significantly decreases the computational effort, making the estimation of SHAP values feasible even as the number of features grows. The use of weighted linear regression in this context helps approximate the impact of each feature within the sampled subsets, providing a practical yet robust method for interpreting complex models.
In this sense Lundeberg and Lees proposed Kernel SHAP works in the following way.  
For each instance $x_j$ a random coalition is generated by repeatedly flipping a coin, resulting in a vector of 0s and 1s. Here, '0' indicates a feature is absent, and '1' indicates it is present; for example, the vector $(1,1,0,0)$ represents a coalition consisting of the first two features.
The process involves sampling K such coalitions, which serve as the data for a regression model, where the prediction for each coalition is the target variable. For missing features in each coalition, values are randomly sampled from the data, assuming independence between absent and present features. This approach helps to simulate the marginal distribution of the features.
After generating predictions for each coalition, a weighted linear regression model is estimated for every observation. The weighting scheme in Kernel SHAP prioritizes extreme coalitions—those with many features present or absent—since these provide the most information about individual feature effects in isolation. The linear regression model is then trained by optimizing a specific loss function, and the resulting coefficients represent the SHAP values for each feature.
A detailed explanation and explicit formulation of the Kernel SHAP estimation method are provided in the Appendix.
While Kernel SHAP is an innovative approach to explaining machine learning model predictions, it has practical limitations that make it less viable for large datasets. Specifically, the computational time required for Kernel SHAP increases substantially as the number of features and instances grows, rendering it impractical for large-scale applications. This slow performance stems primarily from the necessity to sample and evaluate numerous feature coalitions to estimate SHAP values accurately.
Additionally, Kernel SHAP's methodology of sampling from the marginal distribution of missing features presents another critical limitation. This approach assumes feature independence and neglects any underlying dependency structures between features. Such simplification can lead to misleading results, particularly in datasets where features are correlated or exhibit significant interactions. By relying on this assumption, Kernel SHAP may generate and give undue importance to unlikely data instances within the sampled coalitions, potentially skewing the model interpretation and leading to unreliable conclusions.
Given these known weaknesses and the availability of more robust alternatives, such as SHAP permutation, which better handles feature dependencies and scales more effectively with larger datasets, Kernel SHAP will not be employed in this paper. These alternatives are now often preferred in contemporary applications due to their improved accuracy and practicality.

\section{Tree SHAP}
As mentioned above many model-agnostic approaches require sampling from the marginal distribution from the missing features, here potential correlations and interactions between features are often ignored. Therefore, the explanations can vary widely from one sample to another because different samples may suggest different relationships or importance of features. This inherent variability can be problematic, especially when the goal is to derive reliable, consistent insights about how features influence model predictions. High variability can lead to less confidence in the explanations, making it difficult to use them as a basis for decision-making, particularly in critical applications.
\subsection{TreeSHAP advantages}
Popular used current feature attribution methods for tree ensembles are inconsistent; Thats why we need SHAP. In contrast, SHAP values consistently attribute feature im- portance, better align with human intuition, and better recover influential features.  We further defined SHAP inter- action values as a consistent way of measuring potentially hidden pairwise interaction relationships. 
SHAP is difficult to compute (Kernel SHAP). The new algorithm reduces the complexity of computing exact SHAP values from $O(TL2^M)$ to $O(TLD^2)$, this exponential reduction in complexity allows predictions to be explained in seconds. Moreover next to classical feature Dependence Plots and importance plots we now get SHAP dependence and summary plots. 
And current attribution methods cannot represent interactions, so therefore they propose SHAP interaction values. 
\subsection{Key Problems}
\begin{itemize}
\item Model agnostics are very slow (now polynomial instead of exponential time)
\item Model agnostic have sampling variability
\item Current tree specific individualized explanation method are inconsitent. Several common feature attribution methods for tree ensembles are inconsistent, meaning they can lower a feature’s assigned im- portance when the true impact of that feature actually increases. This can prevent the meaningful comparison of feature attribution values. 
\end{itemize}

\subsection{How it works}
A nove approach to estimate SHapley values in a polynomial instead of exponential time. Therefore they propose an algorithm that runs in $O(TLD^2)$ time (we're saying that its runtime can be expected to increase linearly with the number of trees (T), linearly with the number of leaves (L), but quadratically with the depth of the trees (D). The quadratic relationship with depth means that if you increase the depth of the trees, the time it takes to run TreeSHAP will go up by the square of that increase. This is significant because it tells us that deeper trees will disproportionately increase the computation time for SHAP values.)
The intuition of the polynomial time algorithm is to recursively keep track of what proportion of all possible subsets flow down into each of the leaves of the tree. 
\subsection{SHAP summary plot}
Standard feature importance bar charts give a notion of relative importance in the training dataset, but they do not represent the range and distribution of impacts that feauture has on the models output, and how the feature vale relates to its impact. SHAP summary plots leverage individualized feature attributions to convey all these aspects of a features importance while remaining visually concise. 
The density of the age plot shows how common different ages are in the dataset, and the coloring shows a smooth increase in the model’s output (a log odds ratio) as age increases.
\subsection{SHAP dependence plot}
represent the expected output of a model when the value of a specific variable is fixed. The values of the fixed variables are varied and the resulting expected model output is plotted. Plotting how the expected output of a function changes as we change a feature helps explain how the model depends on that feature.
SHAP values can be seen as a rich alternative to partial dependence plots. It give insights in how the features attributed importance changes as its values varies. While standard partial dependence plots only produce lines, SHAP depen- dence plots capture vertical dispersion due to interaction effects in the model. These effects can be visualized by coloring each dot with the value of an interacting feature. A figure for instance shows that it is more alarming to have high blood pressure when you are young. Combining SHAP dependence plots with SHAP interaction values can reveal global interaction patterns. An interaction valze of blood pressure and age accounts for instance fro most of the vertical variance in the systolic blood pressure SHAP values. 

\section{Draft}
As mentioned earlier when trying to explain models predictions of tree ensemble methods we can differentiate between local and gloabl model agnostics. We already presented the permutation feature importance method by Brieman when it comes to the importance of features in a model that can be used. On the local side there arent many established methods to specififcally explain a prediction of a tree ensemble methdos.
The Saabas method that can be used to explain predictions of tree ensemble methods by measuring the change in the models expected output have been proved inconsistent.
Meaning they can lower a features assigned importance when the true impact of that feature actually increases. This can prevent the meaningful comparison of feature attribution values and (at the same time violates one of the pre defined SHAP axioms that ensure a fair distribution). Of the methods we consider, only SHAP values and permutation- based methods are consistent. With their tree desirable properties (Local Acurracy, Missingness, Consistency) they provide an unique and fair solution.
Therefore, it is a stong motivation to use SHAP values, for tree ensemble feature attribution, as shown by Lundberg Lee it eliminiates the significant constistency problem displayed above. 
Despite the advantages as mentioned before we face certain issues when calculating SHAP values.
As mentioned earlier many model agnostic explanation methods can be applied to explain the prediciton of tree ensemble models. The above mentioned KernelSHAP has to sample the values of the absent features from the marginal distribution and therefore potentially ignores any correlations and interactions between the features. This can lead to varying explanations for an instance as different samples now may suggest different relationships or importance of features. The inherent variability can be problematic, especially when the goal is to derive reliable and consistent insights about how features influence model predictions. \\
Moreover, these model agnostic methods (Kernel SHAP and what else) to explain single instances often are very slow, especially when we face tabular data with a range of features and observations. This can be very challenging to compute in many frameworks. \\
TreeSHAP estimates predictions by recursively leveraging the tree structure, evaluating paths that either include or exclude a specific feature. The expected model output is calculated by averaging all possible outcomes, conditioned on the known values of included features. This algorithm fundamentally relies on conditional expectations to compute SHAP values, allowing it to manage correlations among features more subtly compared to KernelSHAP, which samples from marginal distributions and thus assumes feature independence.
In practice, for a single tree and an instance i with a feature subset 
S, if we condition on some but not all features, predictions from unreachable nodes are disregarded. A node is considered unreachable if the decision path leading to it includes a feature not in our subset 
S.  From the remaining terminal nodes, predictions are then averaged, weighted by node sizes—which reflect the number of training samples in each node. The mean of these weighted predictions from the remaining terminal nodes yields the expected prediction for x given S.
TreeSHAP must apply this procedure across all possible subsets of the feature values, effectively propagating all possible subsets 
S simultaneously down to the terminal nodes while keeping track of the number of subsets at each decision node. This method not only ensures a thorough exploration of feature interactions but also maintains computational efficiency by utilizing the inherent structure of decision trees.
\subsection{Problems}
This approach, however, has been pointed out to sometimes assign non-zero SHAP values to features that do not influence the outcome directly, especially when such features are correlated with influential features.


















\section{maybe use/ not sure yet}

	SHAP (SHapley Additive exPlanations), first introduced by \cite{10.5555/3295222.3295230}, is a model-agnostic method designed to assess feature importance by providing insights on both local and global scales. Distinct from methods like permutation feature importance—which offers only a global perspective—SHAP facilitates contrastive explanations, allowing individual predictions to be directly compared both among themselves and against the average prediction of the dataset. This unique ability for detailed comparisons distinguishes SHAP from other global methods, such as feature permutation importance, and local, model-agnostic approaches like LIME.
	
	\subsection{Info}
	Originally in SHAP then the value functions for a model f and a instace i is defiend as the difference between a certain prediction an dthe avergae prediction across all instances. 
	
	\subsection{The two main issues in SHAP for machine learning}
	In machine learning, the model's prediction function 
	f typically requires a complete feature vector to make a prediction. When we want to evaluate the impact of a subset of features, we don't have predefined predictions for every possible configuration of these subsets because the model's output depends on all features. Therefore, we cannot directly apply the Shapley value formula, which requires the value (or prediction) of every possible coalition (or subset) of features. To overcome this, SHAP values use the expected prediction, which is calculated by integrating over the probability distribution of the unspecified features. And second: For a model with a large number of features, the number of possible coalitions is $2^n$
	, where n is the number of features. For each coalition, we would need to calculate the contribution to the prediction with and without each feature, which becomes computationally infeasible as n grows large. SHAP addresses this by sampling subsets and estimating the average marginal contributions across these samples, rather than exhaustively computing them for every possible subset.
	
\end{document}