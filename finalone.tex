\documentclass[12pt]{article}

\usepackage[a4paper, left=2.5cm, top=2.5cm, right=2.5cm, bottom=2cm]{geometry}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage{hyperref} % For hyperlinks
\usepackage{graphicx} % For images
\usepackage{tabularx} % For flexible-width columns
\usepackage{amsmath}
\usepackage{amssymb}


\begin{document}
	\clearpage % or \bigskip, depending on your preference
	\begingroup
	\fontsize{10pt}{12pt}\selectfont
	\setstretch{1.1}
\begin{abstract}
	Recent advancements in machine learning, particularly in ensemble tree models such as random forests, have markedly advanced predictive analytics. However, the complexity of such models often hinders their understanding and interpretability, posing a substantial barrier to their practical application in real world scenarios. 
	This study adresses these concerns by exploring the application of SHAP (Shapley Additive exPlanations) a method grounded in the principles of cooperative game theory.
	By utilizing the California House Price Dataset, this research assesses feature importance within a random forest model, emphasizing a predictive regression task. 
	Our approach is dual-faceted: we aim to not only enhance the interpretability of complex machine learning models but also to refine the accuracy of their predictions. 
	To this end, we introduce the model-agnostic approach using Kernel SHAP and extensively apply the TreeSHAP method, specifically designed for tree-based models.
	Our findings reveal that TreeSHAP not only improves the interpretability of complex machine learning models but also facilitates a more transparent understanding of their outputs.
	This advancement is crucial, especially in domains where clarity and trust in model predictions are vital. 
\end{abstract}
\endgroup
\newpage
\setstretch{1.5}
\section{Introduction}
\textbf{In todays era machine learning -a dynamic field that intergrates and unifies computer science, statistics and information theory for predictions based on data, is rapidly growing and becoming integral to our daily lives. }% the power of trees 
In this context, tree based machine learning models, such as random forests or gradient boosted tree models, with their innate ability to learn from data and improve over time, have transformed problem solving across numerous domains.
Their unparalleled versatility and adaptability, for instance when it comes to handling nonlinear data and effortlessly managing heterogeneous data types (both numerical and categorical), have significantly minimized human error and enhanced efficiency across numerous fields. %black box models
However, their complexity often results in a ”black box” phenomenon \textbf{Zitat}, where the rationale behind decisions remains obscure.
This lack of transparency challenges our understanding of model predictions, the importance of features, and their overall influence on outcomes.
The absence of clear explanations can pose significant risks; without transparent explanations, individuals might draw their own, potentially incorrect, conclusions about how these models make predictions. Furthermore, it could hinder researchers and developers from examining and debugging algorithms effectively, which could lead to the deployment of biased models \textbf{Zitat}.
Therefore, the challenge today lies not just in developing advanced and accurate algorithms, but also in ensuring that these algorithms' outcomes are interpretable and accessible to humans.
This lack of transparency in complex models presents individulas with a crucial dilemma: choosing simpler models for detailed insights into the models’ predictive mechanisms or accepting robust and accurate performance without understanding the underlying logic. \cite{molnar2022}
This trade-off is particularly crucial in sectors with significant consequences, such as finance, healthcare, and criminal justice.
For instance, in finance understanding feature importance in credit scoring and fraud detection is critical for ensuring transparent and fair decision-making \cite{KVAMME2018207}.
In healthcare, insights into feature values that influence predictions are vital, not only for diagnosing diseases but also for identifying contributing factors essential in developing personalized treatment plans \cite{Elish2020ASO}.
Furtheremore, in critical areas like criminal justice and environmental science, interpreting model contributions to decision-making can aid in identifying and mitigating biases, fostering trust in models designed to have long-term impacts on society and the planet  \textbf{Zitat}.\\
As a consequence, model interpretability often is strongly connected to model selection, directly impacting performance outcomes.
Nowadays in various industries the lack of interpretability of complex models still enable stakeholders to favor simpler alternatives, which may not completly capture inherent dependencies and interactions in the data.
As a consequence this can lead to a significant oversight of the data structure ending up in a diminished prediction accuracy. \\
In response to those challenges, specialized fields such as Explainable Artificial Intelligence (XAI) and Interpretable Machine learning (IML) have emerged \textbf{Zitat}.
These disciplines are dedicated to developing frameworks that demystify complex models, making them more accessible. 
The existing research that specialized in interpreting complex models can be distinguished between global and local interpretability \cite{molnar2022}. \textbf{Global interpretability referes to the ability to undertsand the overall behaviour of a model across all instances. On the other hand, local interpretability focuses on explaining the decission-making process for individual predictions, providing insight into the reasoning behind specific model outputs.}
For tree ensemble models like random forest, two mehtods that have been frequently used to inteprete models  and to assess feature importance on a global scale are permutation feature importance \cite{article} and partial dependence plots \cite{4a848dd1-54e3-3c3c-83c3-04977ded2e71}. 
While those approaches provide valuable insights at a global level, they often fall short in explaining individual predicitons.
Similarly, model agnostic methods such as LIME \cite{10.1145/2939672.2939778} and Explanation vectors, although effective in local interpretability, \textbf{may not fully capture the comprehensive behaviour of complex models, especially when it comes to understanding the contribution of each feature across all instances.}
This highlights the need for a framework that bridges the gap between local and global understanding while beeing adaptable across various model architectures. 
In response, SHAP (SHapley Additive exPlanations), a method leveraged from the Shapley values concept of cooperative game theory emerged as a powerfull tool to interprete machine learning models both on a local and a global scale. 
Shapley Values introduced by Lloyd Shapley in 1953 offer a fair solution in cooperative game theorie when adressing the issue of fairly distributing a payout among participants who have contributed differently to the payout.
What distinguishes Shapley values from other methods is their foundation on four core axioms that ensure a fair distribution. These principles guarantee that the contribution of each feature to a model's prediction is allocated justly, mirroring the fair payout distribution among players in a game. 
The initial proposal to adapt Shapley values for interpreting machine learning predictions by detailing feature contributions was made in 2010 \cite{article2} and then further refined in 2014 \cite{article3}.
However, the practical application of these works, nevertheless was limited due to the lack of accessible implementation and the computational intensity of the methods. 
In 2017 then a significant advancement occurred with Lundberg and Lee's work \cite{10.5555/3295222.3295230}, which propelled Shapley Additive exPlanations (SHAP) into the spotlight.
In SHAP the authors found a new model agnostic approach with the Kernel SHAP algorithm, inspired by the LIME framework \cite{10.1145/2939672.2939778}, which employs weighted linear regression with a kernel function to optimize data point weighting and thereby addressed the computational hurdles previously encountered.
Additionally, the implementation of DeepLIFT, specifically tailored for deep learning models, addressed previous computational challenges in deep learning areas. These developments marked a pivotal moment, offering practical, model-agnostic tools that broadened the accessibility and applicability of Shapley values in the field.
In 2020, the development of SHAP advanced significantly with the introduction of TreeSHAP, a model-specific method tailored for explaining tree-based and ensemble tree models locally  \cite{unknown}. 
This algorithm makes it feasible to provide rapid and detailed explanations by using conditional expectations rather than marginal distribution sampling and achieves polynomial time complexity, enabling fast and precise explanations for these popular models \cite{lundberg2019consistent}.\\
As tree based ensemble models are renowed for their accuracy in handling tabular data \cite{Chen_2016} \cite{grinsztajn2022treebased} \cite{lundberg2019explainable} but at the same time still pose challenges in terms of intepretability especially at the local level.
To address still present  interpretability challenges, I will utilize the TreeSHAP algorithm, which is designed to enhance understanding of feature importance using SHAP dependency and summary plots. These tools will aid in uncovering general and potentially hidden patterns within the data.
Therefore,  I will be using a random forest model to predict house prices using the California Housing Prices dataset from \href{https://www.kaggle.com/datasets/camnugent/california-housing-prices}{Kaggle} and featured in \href{https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/}{Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow}.
TreeSHAP, available in the R TreeSHAP package, is favored for its consistency and computational efficiency, which are critical given the dataset's complexity—featuring 14 variables and over 20,000 observations.
Moreover, over other prior heuristic mehods such as the Saabas method used to explain predictions from tree-based ensemble models it is not considered inconsistent \cite{lundberg2019consistent}.
Next to enhancing interpretability and feature importance TreeSHAP in my context will be used to improve to predcition accuracy.
Therefore, I will explore whether insights derived from TreeSHAP's SHAP value importance plots can guide feature selection and thereby potentially enhance the performance of the random forest model.
In this sense the approach in this paper is dual faceted and aims not only to detect hidden patterns in the data, at the same time it aims to use the gained insights to improve prediction accuracy. 
For the following the paper is structure as follows. In the next section I will introduce my feature framework and mention a few changes I made to handle high correlated features.
AfterwardsI will be introducing a random forest model and the mechanism behind the splitting process in a decission tree.
Special emphasis will be hold on the bagging process, that can be seen as the base for reducing overfitting in such a complex model. In section 4 I will present a often used model agnostic method used to assess feature importance with the Permutation feature importance and therby emphasize on the problems that emerge in this tool specially when we have to handle correlated features.  In section 5 then I will be introducing the concept of SHapley values based on an made up example and then swap over to the implication of SHapley values in machine learning.
There we will later brierly explore KernelSHAP, particularly its strategy of sampling predictions from marginal contributions, which, while efficient, may introduce inaccuracies when features are correlated but assumed to be independent. As a solution I will be presenting the treeSHAP algrithm and briefly expalin how this algorthm allows to calculate SHAP values in polynomial time.
 Despite its advantages, I will also discuss some limitations associated with the use of conditional expectations within TreeSHAP that could affect the reliability of the algorithm. In the end we will come to my findings and then figure out whether SHAP helped enhancing the prediction accuracy.
\section{The variable framework}
The real estate sector is a fundamental component of the global economy, as it serves botth as a barometer for economic health and a catalyst for economic growth. Any fluctuations in housing prices can result in extensive implications, influencing macroeconomic policy, individual investment decisions, and givernment housing strategies. This thesis aims to elucidate the dynamics of the housing market by leveraging advanced an advanced machine learning model such as the random forest.
Therefore I will employ a random forest model to analyze a Calfornia Housing data set which is derived from the 1990 California census and is publicly available on \href{https://www.kaggle.com/datasets/camnugent/california-housing-prices}{Kaggle}.
My decission to opt for this dataset\footnote{The exact definition of the variables can be seen in the Appendix  subsection \ref{tab:variables}} is due to its blend of simplicity and complexity, offering a detailed perspective on diverse factors influencing California’s housing markets, including geographic location (longitude and latitude), housing age, size (total rooms and bedrooms), occupancy (population and households), economic status (median income), property value (median house value), and proximity to the ocean.
Prior making predictions and enhancing the interpretability of the random forest using SHAP, it is essential to first examine the datasets structure and labeling. This preliminary analysis will inform any necessary data cleaning or feature engineering steps
	
	
	
	
	
	
	

\section{Appendix}
\subsection{Variables I used}
\begin{table}[ht]
	\centering
	\begin{tabular}{ll}
		\toprule
		\textbf{Variable Name} & \textbf{Description} \\
		\midrule
		Longitude & Geographic coordinate defining the westward measure \\
		Latitude & Geographic coordinate defining the northward measure \\
		HousingMedianAge & Median age of homes in a district \\
		TotalRooms & Aggregate number of rooms in homes \\
		TotalBedrooms & Aggregate number of bedrooms in homes \\
		Population & Total population in a district \\
		Households & Total households in a district \\
		MedianIncome & Median household income (in \$10,000 units) \\
		MedianHouseValue & Median value of homes (in USD) \\
		OceanProximity & Proximity of homes to the ocean \\
		\bottomrule
	\end{tabular}
	\caption{Description of Variables in the California Housing Dataset}
	\label{tab:variables}
\end{table}
	
\bibliographystyle{plain}
\bibliography{referenzen}
\end{document}